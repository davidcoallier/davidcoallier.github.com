<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: rstats | Data, Science, Startups and Investment]]></title>
  <link href="http://davidcoallier.com//blog/categories/rstats/atom.xml" rel="self"/>
  <link href="http://davidcoallier.com//"/>
  <updated>2013-05-07T13:14:21+01:00</updated>
  <id>http://davidcoallier.com//</id>
  <author>
    <name><![CDATA[David Coallier]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[TechCrunch Disrupt Finalists Sentiment Analysis with R]]></title>
    <link href="http://davidcoallier.com//techcrunch-finalists-sentiment-with-r"/>
    <updated>2013-05-01T15:44:00+01:00</updated>
    <id>http://davidcoallier.com//techcrunch-finalists-sentiment-with-r</id>
    <content type="html"><![CDATA[<h1>TechCrunch Finalists Twitter Sentiment Analysis</h1>

<p>Earlier today, I made an experiment to see what could be achieved in 1-hour (Timer next to me). I ended up
with <a href="http://davidcoallier.com/experiments/disrupt-story/">a timeline of tweets for each finalist</a> but on day 2 of
TechCrunch Disrupt, many people seemed interested in my generated plots relating to the "Layers of Experience" pitches.</p>

<p>There are many ways to analyse sentiment from Tweets. One can approach the analysis using Python's NLTK
and by using Naive Bayes Classifiers. That's what we use at <a href="https://engineyard.com">Engine Yard</a> for more complex, and complete analyses.</p>

<p>However, today I'm hoping to show you how accurate a simple word weighing analysis can be. Please do not take this with the hope
of predicting market fluctuations and prediction stock prices. This is merely an example of sentiment analysis, with <strong>R</strong>.</p>

<p><strong>Warning: this is scary-accurate despite its dumbfounding simplicity</strong></p>

<h2>Setup your R</h2>

<p>You will need a few packages in order to get started with <strong>R</strong> and this analysis.</p>

<p>Here are the packages you'll need to include. If you don't have them installed, go ahead and install them with <code>install.packages(...)</code>:</p>

<p><div><script src='https://gist.github.com/5496106.js?file=a-headers.r'></script>
<noscript><pre><code>require('twitteR')

require('RJSONIO')
require('RCurl')
require('stringr')
require('plyr')
require('Unicode')

require('ggplot2')
require('doBy')

require('ROAuth')</code></pre></noscript></div>
</p>

<h2>Authentication with Twitter</h2>

<p>As of API version 1.x with Twitter, you will need to authenticate your requests. Thanfully, R makes is <em>easy</em>, however you will need to interact
with the Twitter website after the handshake.</p>

<p>In order to authenticate with twitter you'll need to go to <a href="https://dev.twitter.com">dev.twitter.com</a> and get an account. From which you will then
create an application and receive a <code>consumer-key</code>, a <code>consumer-secret</code>, an <code>oauth-key</code> and an <code>oauth-secret</code>.</p>

<p>The <strong>R</strong> package we use, will only make use of <code>consumer-key</code> and <code>consumer-secret</code>.</p>

<p>Let's register our Twitter authentication:</p>

<p><div><script src='https://gist.github.com/5496106.js?file=a-twittercred.r'></script>
<noscript><pre><code>consumerKey &lt;- 'zing'
consumerSecret &lt;- 'zong'
reqURL &lt;- 'https://api.twitter.com/oauth/request_token'
accessURL &lt;- 'https://api.twitter.com/oauth/access_token'
authURL &lt;- 'https://api.twitter.com/oauth/authorize'
twitCred &lt;- OAuthFactory$new(consumerKey=consumerKey,
                             consumerSecret=consumerSecret,
                             requestURL=reqURL,
                             accessURL=accessURL,
                             authURL=authURL)

twitCred$handshake()
registerTwitterOAuth(twitCred)</code></pre></noscript></div>
</p>

<h2>A few helper functions</h2>

<p>The next methods we set in <strong>R</strong> are not intrinsically complicated but they make our lives easier as we process the tweets that we retrieve. We
first have a <code>sentiment.words</code> function that scans a file and makes a returns a list of words.</p>

<p><div><script src='https://gist.github.com/5496106.js?file=a-scan-words.r'></script>
<noscript><pre><code>sentiment.words &lt;- function(file) {
  return(scan(file, what=&quot;character&quot;, comment.char=&quot;;&quot;))
}</code></pre></noscript></div>
</p>

<p>This will be useful shortly as we'll load a list of <a href="https://gist.github.com/davidcoallier/5496106/raw/c2e46029bfc67edc07b9bdab560878e0e78d9a72/positive-words.txt">positive</a> and <a href="https://gist.github.com/davidcoallier/5496106/raw/daf29b28ea9eb823522093a1681f52aa06baf2f2/negative-words.txt">negative</a> words for our analysis.</p>

<p>Another function that will come in handy is the <code>ucfirst</code> function that returns the first character of every word as upper-case. This is something I like to use for the presentation phase.</p>

<p><div><script src='https://gist.github.com/5496106.js?file=a-ucfirst.r'></script>
<noscript><pre><code>ucfirst &lt;- function(txt) {
  uc_text &lt;- paste(
    u_to_upper_case(substring(txt, 1, 1)), 
    substring(txt, 2),
    sep=&quot;&quot;, collapse=&quot;&quot;
  )
  
  return(uc_text)
}</code></pre></noscript></div>
</p>

<p>This function has a twist. It handles <strong>unicode</strong> strings so that your analysis doesn't <em>shit the proverbial bed</em>.</p>

<p>The last <strong>utility</strong> function I like to use is one called <code>getText</code>. I'm pretty sure this could be done differently but I've had that since my early days of R and haven't looked into changing it (Not broken, don't change it).</p>

<p><div><script src='https://gist.github.com/5496106.js?file=a-gettext-callback.r'></script>
<noscript><pre><code>##
# Another helper for the laply 
# Oh lazy dave when you'll re-read this code.
##
getText &lt;- function(txt) {
  txt$getText()
}</code></pre></noscript></div>
</p>

<h2>Load Positive And Negative Words</h2>

<p>The next phase is pretty straight forward. We've setup our framework for loading files and parsing their words. In order to move ahead, we need to load <a href="https://gist.github.com/davidcoallier/5496106/raw/c2e46029bfc67edc07b9bdab560878e0e78d9a72/positive-words.txt">positive</a> and <a href="https://gist.github.com/davidcoallier/5496106/raw/daf29b28ea9eb823522093a1681f52aa06baf2f2/negative-words.txt">negative</a> words.</p>

<p><div><script src='https://gist.github.com/5496106.js?file=a-load-words.r'></script>
<noscript><pre><code>sentiment.words.negative &lt;- sentiment.words('negative-words.txt')
sentiment.words.positive &lt;- sentiment.words('positive-words.txt')</code></pre></noscript></div>
</p>

<p>At this point, we are authenticated on Twitter, and we have a list of both positive and negative words.</p>

<h2>Manually adding more words</h2>

<p>When looking at the techcrunch data, the words used to describe companies and startups aren't the same as you'd get when parsing academic essays or more "professional" blogs, therefore, we need to add a few words to our list of positives and negatives:</p>

<p><div><script src='https://gist.github.com/5496106.js?file=a-pos-neg-load.r'></script>
<noscript><pre><code>##
# This list of words doens't have all I need.
# 
# We just add 'floored' and 'disrupt' to both positive and negative so they negate each other.
##
pos.words &lt;- c(sentiment.words.positive, &quot;epic&quot;, &quot;amazeballs&quot;, &quot;sick&quot;, 'floored', 'disrupt')
neg.words &lt;- c(sentiment.words.negative, &quot;wtf&quot;, &quot;epicfail&quot;, &quot;stoopid&quot;, &quot;damn&quot;, 'floored', 'disrupt')</code></pre></noscript></div>
</p>

<p>You might notice that we are adding <em>floored</em> and <em>disrupt</em> to both positive and negative lists. It is on purpose so they negate each other.</p>

<h2>Scoring Each Tweet</h2>

<p>The next step is to <em>search</em> for tweets, and then assign a value to each tweet. Before searching, we'll setup our <code>sentiment.score</code> function which receives a list of
<em>sentences</em> in our case, a list of tweets, we iterate over each sentence, remove punctuations, remove <em>cntrl</em> characters and digits, and finally turn them into lowercase.</p>

<p>Once we've cleaned each tweet, we split the sentence into <strong>words</strong> and calculate the sum of positive against negative word matches. So if a sentence contains 3 positives words, and 1 negative word, its score is <strong>2</strong>.</p>

<p><div><script src='https://gist.github.com/5496106.js?file=a-score-words.r'></script>
<noscript><pre><code>## 
# Mostly used Jeffrey Breen's code however 
# added unicode support for words and right now l
# in the process of adding multi-language support.
##
sentiment.score &lt;- function(sentences, pos.words, neg.words, companyName, .progress='none')
{  
  scores = laply(sentences, function(sentence, pos.words, neg.words) {
    
    sentence &lt;- gsub('[[:punct:]]', '', sentence)
    sentence &lt;- gsub('[[:cntrl:]]', '', sentence)
    sentence &lt;- gsub('\\d+', '', sentence)
    
    # We need this for broken tweets with random chars. 
    sentence = try(u_to_lower_case(sentence), TRUE)
    
    # We need to implement an iconv language identification
    # to then load the proper words lists. Right now, 
    # it only supports english.
    word.list = str_split(sentence, '\\s+')
    
    # Second level lists are teh suck. Unlist all of the things.
    words = unlist(word.list)
    
    pos.matches = match(words, pos.words)
    neg.matches = match(words, neg.words)
    
    # They are not nothing.
    pos.matches = !is.na(pos.matches)
    neg.matches = !is.na(neg.matches)
    
    score = sum(pos.matches) - sum(neg.matches)
    
    return(score)
  }, pos.words, neg.words, .progress=.progress )
  
  scores.df = data.frame(score=scores, text=sentences, name=companyName)
  return(scores.df)
}</code></pre></noscript></div>
</p>

<p>When we have processed each tweet in a search, we basically end up with a <code>data.frame</code> containing the scores, tweets and the company name associated to it.</p>

<h3>Search for Tweets and Score em'</h3>

<p>All processing functions are setup, we can now search for tweets for each of the <strong>TechCrunch Finalists</strong>:</p>

<p><div><script src='https://gist.github.com/5496106.js?file=a-searchtwitter.r'></script>
<noscript><pre><code>enigma.tweets &lt;- searchTwitter('enigma_io', n=1000)
enigma.text   &lt;- laply(enigma.tweets, getText)
enigma.feel   &lt;- sentiment.score(enigma.text, pos.words, neg.words, 'Enigma.io')

healthyout.tweets &lt;- searchTwitter('healthyout', n=1000)
healthyout.text   &lt;- laply(healthyout.tweets, getText)
healthyout.feel   &lt;- sentiment.score(healthyout.text, pos.words, neg.words, 'Healthy Out')

handle.tweets &lt;- searchTwitter('@handle', n=1000)
handle.text   &lt;- laply(handle.tweets, getText)
handle.feel   &lt;- sentiment.score(handle.text, pos.words, neg.words, 'Handle')

floored.tweets &lt;- searchTwitter('floored3d', n=1000)
floored.text   &lt;- laply(floored.tweets, getText)
floored.feel   &lt;- sentiment.score(floored.text, pos.words, neg.words, 'Floored 3D')

supplyshift.tweets &lt;- searchTwitter('supplyshift', n=1000)
supplyshift.text   &lt;- laply(supplyshift.tweets, getText)
supplyshift.feel   &lt;- sentiment.score(supplyshift.text, pos.words, neg.words, 'Supply Shift')

zenefits.tweets &lt;- searchTwitter('zenefits', n=1000)
zenefits.text   &lt;- laply(zenefits.tweets, getText)
zenefits.feel   &lt;- sentiment.score(zenefits.text, pos.words, neg.words, 'Zenefits')

glide.tweets &lt;- searchTwitter('glideapp', n=1000)
glide.text   &lt;- laply(glide.tweets, getText)
glide.feel   &lt;- sentiment.score(glide.text, pos.words, neg.words, 'Glide App')</code></pre></noscript></div>
</p>

<p>In the case of TCDisrupt, we parse the last 1000 tweets for enigma, healthyout, handle, floored 3d, supply shift, zenefits and glide.</p>

<h2>Scoring the Scores</h2>

<p>Once we've scored each tweet for each company, we want to process an overall aggregate so we can put each company in perspective with others.</p>

<p>One way of doing that, is to identify <em>happy</em>, <em>unhappy</em>, <em>very happy</em> and <em>very unhappy</em> tweets:</p>

<p><div><script src='https://gist.github.com/5496106.js?file=a-setscores.r'></script>
<noscript><pre><code>## Now we do the actual binding.
all.scores = rbind(enigma.feel, healthyout.feel, handle.feel, 
                   floored.feel, supplyshift.feel, zenefits.feel, glide.feel)

## Now we only inspect the happy and very happy and the unhappy to very unhappy people.. as bools.
all.scores$very.pos.bool = all.scores$score &gt;= 1
all.scores$very.pos = as.numeric(all.scores$very.pos.bool)

all.scores$very.neg.bool = all.scores$score &lt;= -1
all.scores$very.neg = as.numeric(all.scores$very.neg.bool)

score.df = ddply(all.scores, c('name'), summarise, 
                 very.pos.count=sum(very.pos), 
                 very.neg.count=sum(very.neg))

# Total of &quot;very&quot; people is positive count + negative count
# we'll need that for our next step where we re-rank companies.
score.df$very.total = score.df$very.pos.count +  score.df$very.neg.count
 
score.df$percentScore = round(100 * score.df$very.pos.count / score.df$very.total)
 
# Or if we want the loosers:
score.df$percentLooser = round(100 * score.df$very.neg.count / score.df$very.total)</code></pre></noscript></div>
</p>

<p>Let's go over this codeblock step-by-step:</p>

<ul>
<li><code>all.scores</code>: We simply aggregate all the scores for each company together,</li>
<li><code>all.scores$very.pos.bool</code>: We construct a list of TRUE/FALSE of all tweets that have an overall value above or equal to 1,</li>
<li><code>all.scores$very.pos</code>: From our boolean list, we find their numeric values (0, 1),</li>
<li><code>all.scores$very.neg.bool</code>: Not unlike <code>very.pos.bool</code>, we find very negative comments by finding comments that are smaller or equals to -1,</li>
<li><code>all.scores$very.neg</code>: From our boolean list, we associate numeric values to each bool (0, 1)</li>
<li><code>score.df</code>: This iterates over each score, and computes the sum for each positive and negative scores per company name,</li>
<li><code>score.df$very.total</code>: This is a sum of the positive and negative counts so we have a <strong>total</strong> value to derive percentages from,</li>
<li><code>score.df$percentScore</code>: This is the percentage calculation for positive tweets,</li>
<li><code>score.df$percentLooser</code>: This is the percentage calculation for the negative tweets.</li>
</ul>


<h2>Comparing Sentiments of Companies</h2>

<p>The next thing we need to do, is try to compare the negative, positive, popularity and tweet sentiments for each company in comparison with the other finalists.</p>

<p>Using <strong>ggplot2</strong>, this is pretty easy, but we also need a custom-made function named <code>multiplot</code> which simply grabs a few ggplots, and stitches them together:</p>

<p><div><script src='https://gist.github.com/5496106.js?file=a-multiplot.r'></script>
<noscript><pre><code># Found online months ago... can't remember where. I'll find it and add due creds.
multiplot &lt;- function(..., plotlist=NULL, cols) {
  require(grid)
  
  # Make a list from the ... arguments and plotlist
  plots &lt;- c(list(...), plotlist)
  
  numPlots = length(plots)
  
  # Make the panel
  plotCols = cols                          # Number of columns of plots
  plotRows = ceiling(numPlots/plotCols) # Number of rows needed, calculated from # of cols
  
  # Set up the page
  grid.newpage()
  pushViewport(viewport(layout = grid.layout(plotRows, plotCols)))
  vplayout &lt;- function(x, y)
    viewport(layout.pos.row = x, layout.pos.col = y)
  
  # Make each plot, in the correct location
  for (i in 1:numPlots) {
    curRow = ceiling(i/plotCols)
    curCol = (i-1) %% plotCols + 1
    print(plots[[i]], vp = vplayout(curRow, curCol ))
  }
}</code></pre></noscript></div>
</p>

<p>And finally, once we have our multiplot, we are ready to split our data into different ggplot objects, and then display them:</p>

<p><div><script src='https://gist.github.com/5496106.js?file=a-plottage.r'></script>
<noscript><pre><code>happy &lt;- qplot(reorder(score.df$name, -score.df$percentScore), score.df$percentScore, fill=I(&quot;orange&quot;), 
               geom=c(&quot;bar&quot;), ylab=&quot;Relative Happiness&quot;, xlab=&quot;Company Ranking&quot;) + theme_bw()

unhappy &lt;- qplot(reorder(score.df$name, -score.df$percentLooser), score.df$percentLooser, fill=I(&quot;brown&quot;), 
                 geom=c(&quot;bar&quot;), ylab=&quot;Relative Unhappiness&quot;, xlab=&quot;Company Ranking&quot;) + theme_bw()

sco &lt;- qplot(reorder(score.df$name, -score.df$very.total), score.df$very.total, fill=I(&quot;blue&quot;), 
             geom=c(&quot;bar&quot;), ylab=&quot;Social Mentions&quot;, xlab=&quot;By Company&quot;) + theme_bw()

multiplot(g, sco, happy, unhappy, cols=2)</code></pre></noscript></div>
</p>

<p><img src="http://hostr.co/file/oYzyhTxLhDME/Rplot15.png" alt="Image1" /></p>

<h2>Conclusion</h2>

<p>From looking at the chart, most tweets are positives. This also seems to reflect the reality from looking at the Twitter search page. The second thing to notice is that <strong>Supply Shift</strong> has a lot less mentions than the others, and this also seems to be reflected when looking at the individual popularity of each <a href="http://techcrunch.com/2013/04/30/supplyshift-helps-companies-understand-the-environmental-impact-of-their-supply-chain/">TechCrunch post</a>.</p>

<p>From looking at the charts, my guess would be the winner will be either Healthy Out or Enigma.io. Healthy Out has a very positive sentiment, but enigma has more mentions but also has a great amount of happiness associated with them.</p>

<h2>Links</h2>

<p>The full code of this blog post can be <a href="https://gist.github.com/davidcoallier/5496106#file-processing-r">found here</a>.</p>

<ul>
<li><a href="http://techcrunch.com/2013/04/30/disrupt-ny-battlefield-2013/">TechCrunch Disrupt Battlefield Finalists</a>,</li>
<li>Special thanks to Jeffrey Breen for <a href="http://www.inside-r.org/howto/mining-twitter-airline-consumer-sentiment">his post</a> on Airline Consumer Sentiment,</li>
<li><a href="http://docs.ggplot2.org/current/">ggplot2</a>,</li>
<li>Thanks to <a href="http://hostr.co">Hostr.co</a> for their file hosting service.</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[R: Mapping the Street of Ireland]]></title>
    <link href="http://davidcoallier.com//ye-olde-streets-of-ireland"/>
    <updated>2013-04-22T21:40:00+01:00</updated>
    <id>http://davidcoallier.com//ye-olde-streets-of-ireland</id>
    <content type="html"><![CDATA[<h1>Mapping the Streets of Ireland With R and ggplot2</h1>

<p>Earlier today an interesting article about plotting <a href="http://statistik-stuttgart.de/2013/04/19/r-street-of-france/">the streets of france</a> appeared in my feed. What Simon does in his blog post is essentially read OSM street-lines-data for France and displays it in what I find to be a rather traditional and elegant way.</p>

<p>The first thing I immediately thought of was: "I'd love to see this for Ireland". As an OSX user, it took a few tricks to get the article replicated for Ireland therefore I've decided to share my notes.</p>

<p>Should you wish to replicate this for your country, here are some notes I've taken.</p>

<h2>Get The Data</h2>

<p>The first thing you'll need to do is get data off the <a href="http://www.geofabrik.de/">Geofabrik</a> servers. Being my first time transforming data I didn't know where to look, and what to look for.</p>

<p>Firstly, I found out this URL: http://download.geofabrik.de/europe.html which contains links to <code>.osm.pbf</code>', <code>.shp.zip</code>, and <code>osm.zip</code> files.</p>

<p>Not unlike Simon, you'll have to download the <code>.osm.pbf</code> file understanding that you'll have to convert it to an osm-friendly format. The <code>.shp.zip</code> file contained too many dimensions for me to care to learn how it works.</p>

<h2>Convert The PBF file</h2>

<p>Second step is to unzip, then convert the downloaded <code>pbf</code> file to an <code>o5m</code> file, and finally filter the parts of the <code>osm</code> file you want to work with. Let's see how you do all of that.</p>

<p>After acquiring <a href="http://m.m.i24.cc/osmconvert.c">osmconvert</a> and <a href="http://m.m.i24.cc/osmfilter.c">osmfilter</a>, you'll need to compile them.</p>

<p>For <strong>osmconvert</strong>, it's as simple as:</p>

<pre><code>$&gt; wget -O - http://m.m.i24.cc/osmconvert.c | cc -x c - -lz -O3 -o osmconvert
</code></pre>

<p>And for <strong>osmfilter</strong> again:</p>

<pre><code>$&gt; wget -O - http://m.m.i24.cc/osmfilter.c |cc -x c - -O3 -o osmfilter
</code></pre>

<p>After having compiled <strong>osmconvert</strong> and <strong>osmfilter</strong>, you are ready to begin the conversion of the <code>.pbf</code> file to <code>.o5m</code> to <code>.osm</code>.</p>

<h3>Convert From PBF to o5m</h3>

<p>Using the compiled <code>osmconvert</code>, convert the <code>pbf</code> from <code>o5m</code> like such:</p>

<pre><code>$&gt; ./osmconvert ireland-and-northern-ireland-latest.osm.pbf --out-o5m -o="ireland.o5m"
</code></pre>

<h3>Convert From o5m to osm</h3>

<p>Now using the second compiled program <code>osmfilter</code>, extract the parts you are interested in (Again, this is directly taken from Simon's article):</p>

<p><code>
./osmfilter ireland.o5m --keep="highway=motorway =motorway_link =trunk
=trunk_link =primary =primary_link =secondary =secondary_link =tertiary
=tertiary_link =living_street =pedestrian =residential =unclassified
=service" --drop-author --drop-version &gt; ireland.osm
</code></p>

<h2>QuantumGIS</h2>

<p>Now that you've converted the <code>pbf</code>, and filtered the <code>o5m</code> into a cleaned <code>osm</code>, I needed to transform the <code>osm</code> file into an <code>shp</code> shapefile.</p>

<p>You can get QuantumGIS from http://www.kyngchaos.com/software/qgis. Follow the requirements, and you will be able to open up QuantumGIS.</p>

<h3>QuantumGIS OpenStreetMap Plugin</h3>

<p>Before converting your layers to <code>shp</code> files, you will need to enable the OpenStreetMap Plugin in QGIS.</p>

<ol>
<li>Top menu, select "Plugins" > "Manage Plugins…",</li>
<li>From the Manage Plugins windows, search for "Open" and enable the OpenStreetMap core plugin</li>
</ol>


<h3>Loading OSM file into QuantumGIS</h3>

<p>Now that you've installed the OpenStreetMap plugin, a new "Top Menu" item should have appeared in QuantumGIS. Look for a menu-item named "Web".</p>

<p>From there, you will have multiple choices under the <strong>OpenStreetMap</strong> option. Select <code>Load OSM from file</code> and select the <code>osm</code> file we've created before.</p>

<h3>QuantumGIS Layers</h3>

<p>You're nearly there. Now, on the left-handside, you will see 3 layers: {country}<em>streets_points, {country}</em>streets_line, {country}_streets_polygons.</p>

<p>Unselect points, and polygons. Keep only lines.</p>

<h3>Save as SHP</h3>

<p>Now that you've only the lines for your country/region/area, from the top menu, select <strong>Layer</strong> > <strong>Save As</strong> and save your shp shape file.</p>

<h2>Getting R-eady</h2>

<p>Finally some <strong>R</strong>...</p>

<p>Now, in order to continue, you will need to make sure the following packages are installed:</p>

<ol>
<li>ggplot2</li>
<li>maptools</li>
<li>proto</li>
<li>sp</li>
</ol>


<p>If you don't have them, you can install them with:</p>

<pre><code>R&gt; install.packages(c('ggplot2', 'maptools', 'proto', 'sp'))
</code></pre>

<h2>Loading the Data in R</h2>

<p>You have all the required packages, your files have been converted into the proper <code>shp</code> format, you are now ready to load the data into something that can be displayed:</p>

<p><div><script src='https://gist.github.com/5421734.js?file=z-ggplot-for-blog.r'></script>
<noscript><pre><code>shpIreland &lt;- readShapeLines(&quot;path/to/shapefile/file.shp&quot;)
linesIreland &lt;- conv_sp_lines_to_seg(shpIreland)
rm(shpIreland)

streets &lt;- geom_segment2(data=linesIreland, 
                         aes(xend=elon, yend=elat), 
                         size=.055, 
                         color=&quot;black&quot;)

p &lt;- ggplot(linesIreland, aes(x=slon,y=slat)) + 
    streets + 
    scale_x_continuous(&quot;&quot;, breaks=NULL) + 
    scale_y_continuous(&quot;&quot;, breaks=NULL) +
    labs(title=&quot;Streets of Ireland and Northern Ireland 2013&quot;) +
    theme(panel.background=element_rect(fill='#f5f4e0')) +
    theme(plot.background=element_rect(fill='#f5f4e0')) </code></pre></noscript></div>
</p>

<p>Please note that <code>geom_segment2</code> and <code>conv_sp_lines_to_seg</code> are respectively third-party functions from <a href="http://spatialanalysis.co.uk/2012/02/great-maps-ggplot2/">Spatial Analysis UK</a>, and <a href="http://muon-stat.com/landkarten/convert_shp_line_to_seg.R">Simon Müller</a>. You can find a script attached to this <strong>gist</strong> that contains all the code.</p>

<h2>Resulting ggplot2 Image</h2>

<p>Here's what the end result with a map that displays the sparse streets of Ireland looks like:</p>

<p><img src="http://hostr.co/file/PlFMCIFuKiiT/ireland-streets-small.png" alt="Small Street" /></p>

<h2>Title for Edward R. Tufte</h2>

<p>Now, I've saved the plot as a PDF from RStudio, and opened it with Adobe Illustrator which allows me to edit various parts of the plot. That is how the title has been changed. ( <strong>That's a pretty big tip right there, write that down</strong> )</p>

<p>If you've ever read Edward R. Tufte, you may have come across one of his book called: <strong>The Visual Display of Quantitative Information</strong> which displays very elegant, <code>Ye Olde Plot</code> types of plots. It is the reason why this map has an edited title, even though I didn't have time to find the proper font so I would agree that it looks slightly sloppy.</p>

<p>If you are interested in the original image, please see <a href="http://hostr.co/file/yKZBiz7xww6K/ireland-streets.png">the larger image</a></p>

<h2>Conclusion</h2>

<p>It's fairly simplistic, and I believe it created what is to be an elegant old-style plot of Ireland's streets (which comprises <em>2,133,736</em> street-data points).</p>

<p>Thanks to Simon for his original article.</p>

<ul>
<li>See the code in the file called "plotting-full.R" under,</li>
<li>The data OpenStreetMap-data is published under CC BY-SA-licence</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Load, Shape and Visualise Data]]></title>
    <link href="http://davidcoallier.com//load-shape-visualise-data-with-r"/>
    <updated>2013-04-22T20:55:00+01:00</updated>
    <id>http://davidcoallier.com//load-shape-visualise-data-with-r</id>
    <content type="html"><![CDATA[<h1>Load Data With Hmisc</h1>

<p>Often times when working with data in <code>R</code>, you find yourself needing to summarise a <strong>data.frame</strong> by a column, a
<code>SELECT SUM(..)</code> or <code>SELECT COUNT(..)</code> in MySQL if you will.</p>

<p>For instance, you may have a lot of page-visits to analyse and you aren't quite certain how to summarise some
of that data using <code>R</code>.</p>

<p>Let's consider the following (trivial) dataset:</p>

<p><div><script src='https://gist.github.com/5438020.js?file=1.csv'></script>
<noscript><pre><code>user, created.at, pages.visited
julie, 2011-11-16 03:31:56 GMT, 3
julie, 2011-11-16 04:22:33 GMT, 5
jack, 2011-11-16 07:19:13 GMT, 8
jack, 2011-11-16 12:39:02 GMT, 9
julie, 2011-11-17 12:39:02 GMT, 3</code></pre></noscript></div>
</p>

<p>Let's load this <strong>csv</strong> file into <code>R</code> using a package called <strong>Hmisc</strong>:</p>

<p><div><script src='https://gist.github.com/5438020.js?file=2.r'></script>
<noscript><pre><code>require(Hmisc)
require(plyr)
require(ggplot2)

data &lt;- csv.get('path/to/csv-file.csv', header=TRUE)</code></pre></noscript></div>
</p>

<p>By using <code>csv.get</code> we essentially create a <code>data</code> variable of type  <em>data.frame</em> on which we can start operating.</p>

<h1>Shape the data (Clean)</h1>

<p>The first thing we'll want to do is format the <code>created.at</code> field so that it is parsable and formattable.</p>

<p>For the purpose of this analysis, let's assume that we are based in Los Angeles. If we look closely at the data we have loaded, we
notice the timezone of the <code>data$created.at</code> field being <strong>GMT</strong>.</p>

<p>Considering we are in Los Angeles, visualising <strong>GMT</strong> data will not be as intuitive for our team, therefore we are
going to start by changing the timezone (and values) of the <code>data$created.at</code> to "Los Angeles" times.</p>

<p><div><script src='https://gist.github.com/5438020.js?file=3.r'></script>
<noscript><pre><code>data$created.at &lt;- as.POSIXct(
    format(data$created.at, tz=&quot;America/Los_Angeles&quot; usetz=TRUE)
)</code></pre></noscript></div>
</p>

<p>We now have data that is ready to be parsed, and that is represented with local Los Angeles times. We decide we want
to see how people are using the data per hour of the day.</p>

<p><code>R</code> <em>data.frame</em>s are fairly versatile and allow us to add the hour for each entry as easily as:</p>

<p><div><script src='https://gist.github.com/5438020.js?file=4.r'></script>
<noscript><pre><code>data$hour &lt;- format(data$created.at, '%H')</code></pre></noscript></div>
</p>

<p>Our <code>data</code> now contains the <strong>hour</strong> column and looks like:</p>

<p><div><script src='https://gist.github.com/5438020.js?file=5.csv'></script>
<noscript><pre><code>user          created.at pages.visited hour
1 julie 2011-11-16 03:31:56            3   03
2 julie 2011-11-16 04:22:33            5   04
3 jack 2011-11-16 07:19:13             8   07
4 jack 2011-11-16 12:39:02             9   12
5 julie 2011-11-17 12:39:02            3   12</code></pre></noscript></div>
</p>

<p>At this point we want to count the number of pages visited per hour, per user. If we were using
an SQL based solution, we could simply do something like:</p>

<p>SELECT SUM(pages.visited) as countPerHour, user, hour FROM data GROUP BY user, hour</p>

<p>The only problem, is we aren't using an SQL-based solution, therefore we need to learn how to
manipulate and summarise data from within <code>R</code>.</p>

<p>The <code>plyr</code> package in <code>R</code> allows us to manipulate lists, dataframes, etc. very easily. In this
instance, it also allows ut to summarise as if we were using and SQL-based query.</p>

<p><div><script src='https://gist.github.com/5438020.js?file=6.r'></script>
<noscript><pre><code>byHour &lt;- ddply(data, .(user, hour), summarise, countPerHour=sum(pages.visited))</code></pre></noscript></div>
</p>

<p>We now have a new <em>data.frame</em> <code>byHour</code> which contains the sum of visited page per hour per user.</p>

<p>Now let's say we also want to take a look at the visits by weekday instead of by hour. We'll start by adding
the weekday to our original <code>data</code> <em>data.frame</em> and again, we'll use <code>ddply</code> from <code>plyr</code> to sum/count
the number of visits per weekday:</p>

<p><div><script src='https://gist.github.com/5438020.js?file=7.r'></script>
<noscript><pre><code>data$weekday &lt;- weekdays(data$created.at)
byWeekday &lt;- ddply(data, .(user, weekday), summarise, countPerDay=sum(pages.visited))</code></pre></noscript></div>
</p>

<p>You now have data summarised that is similar to what you would traditionally get with
MySQL by doing <code>SELECT SUM(hour) as countPerHour, user, hour FROM data GROUP BY user, user</code> or even
<code>SELECT SUM(weekday) as countPerDay, user, weekday FROM data GROUP BY user, weekday</code>.</p>

<p>The difference, is that you can start visualising your data a lot faster now that you have a
formatted <em>data.frame</em>.</p>

<h1>Visualise Your Cleansed Data</h1>

<p>As with most analysis, you will want to visualise this data-set. Preferably, you'll generate a few
plots and identify which ones pleases you the most.</p>

<h3>Visited Pages Per Hour</h3>

<p>Lets see how we can use <code>ggplot2</code> to visualise this data-set in a few different ways:</p>

<p><div><script src='https://gist.github.com/5438020.js?file=8.r'></script>
<noscript><pre><code>ggplot(byHour, aes(x=hour, y=as.numeric(countPerHour))) + 
    geom_bar(stat=&quot;identity&quot;) +
    labs(title=&quot;Count per hour for all users&quot;) +
    ylab(&quot;Visits Per Hour&quot;) + xlab(&quot;Hour of the day&quot;) +
    theme_bw()</code></pre></noscript></div>
</p>

<p><img src="https://hostr.co/file/P6yIYUIGBJzK/plot1.png" alt="Plot1" /></p>

<p>Now this seems to be exhibiting an upward trend however with only 2-different users,
our analysis is fairly weak.</p>

<h3>Visited Pages Per Hour Per User</h3>

<p>What if we decide to put the visited page, per user, as columns but this
time we'll put them side-by-side so we can compare the two users:</p>

<p><div><script src='https://gist.github.com/5438020.js?file=9.r'></script>
<noscript><pre><code>ggplot(byHour, aes(x=hour, y=as.numeric(countPerHour))) + 
  geom_bar(aes(fill=user), position=&quot;dodge&quot;) +
  labs(title=&quot;Dodge per hour per user&quot;) +
  ylab(&quot;Visits Per Hour&quot;) + xlab(&quot;Hour of the day&quot;) +
  theme_bw()</code></pre></noscript></div>
</p>

<p><img src="https://hostr.co/file/5t8GYEOJimA9/plot2.png" alt="Plot2" /></p>

<p>From that we can now see that Julie is the only one that visited pages between 3am and 4am. On the other hand,
at 7am, only Jack used the site. At 12pm, both users used the site but Jack used it more.</p>

<p>The interesting part in this is the:</p>

<pre><code>geom_bar(aes(fill=user), position="dodge")
</code></pre>

<p>This tells the plotting-engine to fill bars with colours for each user, but then tells the engine
to <strong>dodge</strong> the bars instead of say <strong>stacking</strong> them.</p>

<p>What this shows is that our upwards trend now shows that only 1 user is using it more as the hours progress
and that Julie is in fact using it less.</p>

<h3>User-Faceted Visited Pages Per Hour</h3>

<p>Another way of visualise this would be to seperate the users into facets as such:</p>

<p><div><script src='https://gist.github.com/5438020.js?file=10.r'></script>
<noscript><pre><code>ggplot(byHour, aes(x=hour, y=as.numeric(countPerHour))) + 
  geom_bar(aes(fill=user), stat=&quot;identity&quot;) +
  labs(title=&quot;Pages visited per hour of day&quot;) +
  ylab(&quot;Visits Per Hour&quot;) + xlab(&quot;Hour of the day&quot;) +
  theme_bw() + facet_grid(user~.)</code></pre></noscript></div>
</p>

<p><img src="https://hostr.co/file/iYp5TasMxCGh/plot3.png" alt="Plot3" /></p>

<p>From this, what you need to look at is:</p>

<pre><code>facet_grid(user~.)
</code></pre>

<p>which splits the plot into 2 facets, each facet being the <code>user</code> field.</p>

<h3>Visited Pages Per Weekday</h3>

<p>Now considering we don't have much data in our example data-set, this will yield fairly
dissapointing results however we'll look at a way to change how the axis is displayed:</p>

<p><div><script src='https://gist.github.com/5438020.js?file=11.r'></script>
<noscript><pre><code>ggplot(byWeekday, aes(x=weekday, y=as.numeric(countPerDay))) + 
  geom_bar(aes(fill=user), position=&quot;dodge&quot;) +
  labs(title=&quot;Dodge per hour per user&quot;) +
  ylab(&quot;Visits Per Hour&quot;) + xlab(&quot;Hour of the day&quot;) +
  theme_bw() + coord_flip()</code></pre></noscript></div>
</p>

<p><img src="https://hostr.co/file/AezmAnWzfHZw/plot4.png" alt="Plot4" /></p>

<p>The interesting part from this snippet is:</p>

<pre><code>coord_flip()
</code></pre>

<p>Which basically takes the x-axis and flips it on its side. In the case of weekdays, we have a maximum
of seven wherein the names can be long ("Wednesday", "Saturday", etc). This makes it a lot easier
to read and visualise.</p>

<p>From the previous plot, we can easily see that the only person that visited pages on Thursday was Julie,
however on Wednesday, Jack visited a lot more pages than Julie. Should we assume that Jack didn't do
any work on Wednesday? I'll make you draw your own conclusions ;-)</p>

<h3>User-Weekday-Faceted Visited Pages Per Hour</h3>

<p>There are many different ways of looking at this type of data and I hope I've conveyed the power of <code>R</code> and the rapidity at
which it enables you to prototype. <code>R</code> makes it very easy to <strong>load</strong>, <strong>manipulate</strong> and <strong>visualise</strong> data rapidly.</p>

<p>Now assuming you had many different weekdays and you wanted to visualise the number of pages visited per day per hour. You could do
something similar to:</p>

<p><div><script src='https://gist.github.com/5438020.js?file=12.r'></script>
<noscript><pre><code>byWeekdayHour &lt;- ddply(data, .(user, hour, weekday), summarise, countPerHour=sum(pages.visited))

byWeekdayHour$weekday &lt;- factor(
    byWeekdayHour$weekday, 
    levels=c('Wednesday', 'Thursday')
)

ggplot(byWeekdayHour, aes(x=hour, y=as.numeric(countPerHour))) + 
    geom_bar(aes(fill=user), stat=&quot;identity&quot;) +
    labs(title=&quot;Pages visited per hour of day&quot;) +
    ylab(&quot;Visits Per Hour&quot;) + xlab(&quot;Hour of the day&quot;) +
    theme_bw() + facet_grid(user~weekday)</code></pre></noscript></div>
</p>

<p><img src="https://hostr.co/file/TfPo8ZPJC0OX/plot5.png" alt="Plot5" /></p>

<p>The first thing we do is we select per user, hour and weekday and sum the visited pages. The second part of the
snippet, the part that contains <code>factor(...)</code> is only so we order the weekdays in the proper order... Wednesday
comes before Thursday therefore we re-order it (Otherwise Thursday comes before Wednesday (T vs W)).</p>

<h2>Conclusion</h2>

<p>For the less experienced <code>R</code> users, keep in mind that load and intensively manipulating data is going to be of
utmost importance when you prototype with R. Visualising is also a very important part of the analysis as it
allows for your cognitive functions to identify patterns before investing time into a deeper analysis.</p>

<p>Now go forth, be crazy.</p>
]]></content>
  </entry>
  
</feed>
